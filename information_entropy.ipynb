{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- some nicer latex formatting -->\n",
       "<style>\n",
       ".jp-MarkdownOutput {\n",
       "    max-width: 40vw;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "        displayAlign: \"left\",\n",
       "        displayIndent: \"2em\",\n",
       "        \"HTML-CSS\": {\n",
       "            availableFonts: [\"TeX\"],\n",
       "            preferredFont: \"TeX\",\n",
       "            webfonts: \"TeX\"\n",
       "        }\n",
       "    });\n",
       "    MathJax.Hub.Queue(\n",
       "        [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "        [\"PreProcess\", MathJax.Hub],\n",
       "        [\"Reprocess\", MathJax.Hub]\n",
       "    );\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<!-- some nicer latex formatting -->\n",
    "<style>\n",
    ".jp-MarkdownOutput {\n",
    "    max-width: 40vw;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<script>\n",
    "    MathJax.Hub.Config({\n",
    "        displayAlign: \"left\",\n",
    "        displayIndent: \"2em\",\n",
    "        \"HTML-CSS\": {\n",
    "            availableFonts: [\"TeX\"],\n",
    "            preferredFont: \"TeX\",\n",
    "            webfonts: \"TeX\"\n",
    "        }\n",
    "    });\n",
    "    MathJax.Hub.Queue(\n",
    "        [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "        [\"PreProcess\", MathJax.Hub],\n",
    "        [\"Reprocess\", MathJax.Hub]\n",
    "    );\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Entropy\n",
    "\n",
    "This notebook is an introduction to information entropy. It tries to bring together various interpretations and uses of entropy, but largely from a machine learning perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "The definition of the entropy of a random variable is:\n",
    "> the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes</br>\n",
    "> -- [Wikipedia](https://en.wikipedia.org/wiki/Entropy_(information_theory)#Definition)\n",
    "\n",
    "This is a little ambiguous. Let's take a look at the formula. The average entropy of a random variable $X$ is given by\n",
    "\n",
    "$$\\mathcal{H}(X) = \\mathbb{E}\\left[ -\\log X\\right]$$\n",
    "\n",
    "Well, this doesn't clarify too much either. Let's go back to first principles and build some intuition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information, Uncertainty, and Entropy\n",
    "\n",
    "Suppose I have written down an integer, $X$, chosen uniformly at random between $1$ and $100$.\n",
    "\n",
    "You don't know what the number is so you are missing some _information_. We want to measure the level of _**missing information**_ which we will call **entropy**. Equivalently, you could say there is a level of **uncertainty** in $X$. These are just both sides of the same coin and we will use these terms interchaneably.\n",
    "\n",
    "Consider two different events:\n",
    "* $A$: $X$ is between $1$ and $50$\n",
    "* $B$: $X$ is between $1$ and $10$\n",
    "\n",
    "If I told you that $B$ was true, then you would know more about the number than if I told you $A$ was true. Indeed, $B \\subset A.$ So by knowing $B$ is true you know $A$ (plus a bit more). This means I must have provided _more_ information by sharing that $B$ was true. Equivalently, the level of uncertainty decreases more if I tell you $B$ is true.\n",
    "\n",
    "On the other hand, if I told you that $B$ was not true, then you would only know that the number is between $11$ and $100$, and I would have provided _less_ information than sharing that $A$ was true.\n",
    "\n",
    "Moreover, if you already knew the number was $5$, then $P(X=5)=1$, there would be no uncertainty, and telling you the number would convey _no_ information. \n",
    "\n",
    "If we look from a probability perspective then\n",
    "* $P(A) = 0.5$\n",
    "* $P(B) = 0.1$\n",
    "* $P(\\text{not}\\, B) = 0.9$\n",
    "\n",
    "Now we can make a key observation about Information Entropy:\n",
    "> **Low probability events contain more information than high probability events.**\n",
    "\n",
    "Or..\n",
    "> **Low probability events have higher entropy than high probability events.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Measure of Information\n",
    "\n",
    "We have some intuition that a probability event, $E$, represents a certain level of (missing) information $I(E)$. We haven't defined any units for information, so at the moment it is just an abstract quantity, and what matters is the relativity of $I$ between certain events. First we will try to posit some properties that we would like $I$ to have, and use them to define $I$.\n",
    "\n",
    "### Existing Information\n",
    "\n",
    "In the previous example there was a key underlying assumption: you knew that $X$ was uniformly distributed between $1$ and $100$. That is, you knew the sample space, the probability distribution and the event $A$. Therefore, the increase in information when event $A$ is realised does not contain information about the sample space itself (because it is already known). \n",
    "\n",
    "To put it another way, the uncertainty of event $A$ is the same as the level of uncertainty in a fair coin toss coming up heads. Both have probability $0.5$ and both in a sense \"halve the sample space\".\n",
    "\n",
    "So even though the sample space of a fair coin toss, $Y$, only has $2$ elements, $\\{H, T\\}$, and the sample space of $X$ has $100$ elements, the _decrease_ in uncertainty / _increase_ in information is the same. The difference between these two events was already existing information.\n",
    "\n",
    "Now we have some intuition about another property of entropy:\n",
    "> **the entropy of an event $E$ does not depend on the sample space, only the probability $p(E)$**\n",
    "\n",
    "That is, there is some function $I^*$ such that \n",
    "\n",
    "$$I(E) = I^*(p(E)) \\tag{*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity\n",
    "\n",
    "Let's now continue to consider the random variable $Y$ representing a (not necessarily fair) coin toss, and the event $H_Y$ that the the coin comes up heads. If $Y$ comes up heads then we have learned $I(H_Y)$ units of information. If we flip another coin $Z$, which also comes up heads then we again learn $I(H_Z)$ units of information. The total information we learned is $I(H_Y) + I(H_Z)$. That is, when two events are independent\n",
    "\n",
    "$$I(E_1 \\land E_2) = I(E_1) + I(E_2)$$\n",
    "\n",
    "However, these events are independent so\n",
    "\n",
    "$$p(E_1 \\land E_2) = p(E_1) p(E_2)$$\n",
    "\n",
    "And combining with equation $(*)$\n",
    "\n",
    "$$I^*(xy) = I^*(x) + I^*(y)$$\n",
    "\n",
    "Where $x,y \\in [0, 1]$. In fact, this is equivalent to [Cauchy's functional equation](https://en.wikipedia.org/wiki/Cauchy%27s_functional_equation) under the substitution $f(x)= g(e^x)$ and there is only one family of (continuous) solutions: \n",
    "\n",
    "$$I^*(x) = \\alpha \\log_e(x)$$\n",
    "\n",
    "Where $\\alpha \\in \\mathbb{R}$. So now we have a definition of $I$\n",
    "\n",
    "$$I(E) = \\alpha \\log_e p(E) \\tag{**}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Observations\n",
    "\n",
    "Notice that our definition of $I$ has a free parameter $\\alpha$. This is because we have not defined any units for information.\n",
    "\n",
    "Typically $\\alpha$ is set to $-1$, which ensures that $I$ is always positive and conforms to the observation that low probability events have higher entropy.\n",
    "\n",
    "If we set $\\alpha = \\dfrac{-1}{\\log_e 2}$ then we change the basis: $I(E) = -\\log_2 p(E)$, and we can make an interpretation of $I$ as the number of bits required to encode the information. This is also called [Shannon Entropy](https://en.wiktionary.org/wiki/Shannon_entropy) after Claude Shannon who introduced this idea. In particular the event of a fair coin coming up heads $I(H) = -\\log_2 0.5 = 1$ bit. We will come back to this shortly.\n",
    "\n",
    "As an aside (feel free to skip this part), setting $\\alpha = k_B = 1.380649\\cdot10^{âˆ’23}$ known as the [Boltzman Constant](https://en.wikipedia.org/wiki/Boltzmann_constant) provides a connection to the interpretation of entropy in physics. This interpretation deals with the configuration of molecules in a system. In essence, if there are some molecules in $W$ possible configurations (microstates) then the entropy is given by $S = k_B \\log_e W$. Now, if each configuration is equally likely then the probability that they are in a specific microstate is $\\frac{1}{W}$, and the entropy is... $-k_B \\log_e \\frac{1}W$. Which looks just like our interpretation of information entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information in Bits\n",
    "\n",
    "Information/Entropy can be described as\n",
    "\n",
    "> the number of bits required to encode and transmit an event.\n",
    "\n",
    "I have so far avoided this analogy because I think it can cause some confusion.\n",
    "\n",
    "For instance, the event $HTHHT$ with respect to a fair coin being flipped 5 times, could be encoded $01001$. This provides some intuition that an event with probability $\\frac1{2^5}$ has entropy $5$.\n",
    "\n",
    "But what about a weighted coin that comes up heads with probability $0.9$ and tails $0.1$. A sequence of $5$ such coin tosses could still be \"encoded\" as $01001$, so we might feel that these have the same entropy. But we already know this event has entropy $I(HTHHT) = -\\log_2(0.9^30.1^2) \\approx 7.1$. _So what is the problem with this intuition?_\n",
    "\n",
    "The problem is that this event has a lower probability than the fair coin equivalent, so it has higher entropy. A more explicit description might be:\n",
    "\n",
    "> **entropy is the number of _units of information_ required to encode and transmit an event**\n",
    "\n",
    "And recall from the previous section that we made this interpretation of encoding bits by setting $\\alpha = \\frac{-1}{\\log_e 2}$. That is, _we_ specified that 1 bit is the amount of information transferred in the realisation of a _fair_ coin toss, not the othe way around!\n",
    "\n",
    "We could just as easily define the realisation of a fair coin toss to be 2 bits or 3.14 bits. But by this definition we get the interpretation of information as a number of bits, like a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy of a Random Variable\n",
    "\n",
    "So far we have only considered single probabilistic events, but not random variables themselves. We have already seen that different realisations of a random variable $X$ can convey different levels of information (as with a weighted coin). So in fact, it doesn't make sense to talk about the information / level of uncertainty / entropy of a random variable $X$ itself, we can only talk about it's _expected entropy_. Or, as we saw in the first section:\n",
    "\n",
    "$$\\mathcal{H}(X) = \\mathbb{E}\\left[-\\log X \\right]$$ \n",
    "\n",
    "So for discrete distributions we have\n",
    "\n",
    "$$\\mathcal{H}(X) = -\\sum_{j} p_j \\log p_j$$\n",
    "\n",
    "And in contradiction to what I just said, we will refer to $\\mathcal{H}(X)$ as the entropy of $X$, even though it's really the expected entropy.\n",
    "\n",
    "So the entropy of a fair coin toss, $Y$ is \n",
    "\n",
    "$$\\mathcal{H}(Y) = -(0.5\\log_2(0.5) + 0.5\\log_2(0.5)) = \\log_2(2) = 1$$\n",
    "\n",
    "And the entropy of a weighted coin $Z$ that comes up heads with probability $0.9$ is\n",
    "\n",
    "$$\\mathcal{H}(Z) = -(0.9\\log_2(0.9) + 0.1\\log_2(0.1)) \\approx 0.47$$\n",
    "\n",
    "Notice that the weighted coin has lower entropy because there is less uncertainty (we are more confient that the coin will come up heads). We're now touching on the idea of maximum entropy. We'll come back to this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Bits\n",
    "\n",
    "Perhaps things still feel a bit abstract. Let's use our current understanding to present a more concrete example for a use of Entropy -- compression.\n",
    "\n",
    "If we want to convert a string of text to a string of bits (i.e. real computer bits), we could use an encoding to first map each character to a representation in bits (not necessarily all the same length), and replace each character with it's binary representation. If we want to do this conversion in an optimal way (i.e. using the least number of bits), then more common characters should be assigned shorter binary representations.\n",
    "\n",
    "The [source encoding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) says that this compression will be optimal when each character's representation in bits has length $-\\log_2 f$, where $f$ is the frequency of the character in the string.\n",
    "\n",
    "Now, clearly you cannot use a fractional number of bits in practice, but the source encoding theorem gives the lower bound for the maximum compression, which is the entropy of the character distribution. Neat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy and Kullback-Leibler Divergence\n",
    "\n",
    "Now let's go back to the initial example of the (discrete) uniform distribution $X \\sim U(1, 100)$. For each integer $1 \\le j \\le 100$, the probability mass is $p_j = \\frac{1}{100}$.\n",
    "\n",
    "Suppose you had a different belief about the distribution of $X$. You assigned each integer a probability mass $q_j$. What would be the expected entropy that arises from using your probability mass $Q$ rather than the true probability mass $P$?\n",
    "\n",
    "Well, each event $j$ still occurs with probability $p_j$, but you assigned a different level of entropy $-\\log_2(q_j)$. The expectation would therefore be\n",
    "\n",
    "$$\\mathcal{H}(P, Q) = -\\sum_j p_j \\log_2(q_j)$$\n",
    "\n",
    "This is referred to as the _cross entropy_ and, as it turns out, this will always overestimate the true entropy $\\mathcal{H}(P)$. That is $\\mathcal{H}(P, Q) \\ge \\mathcal{H}(P)$. The different between the cross entropy and the true entropy is the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), representing how much we have overestimated the entropy by using the incorrect distribution $Q$. That is\n",
    "\n",
    "$$\\mathcal{D}_{KL}(P\\parallel Q) = \\mathcal{H}(P, Q) - \\mathcal{H}(P) = -\\sum_j p_j \\log_2 \\frac{q_j}{p_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence is always positive\n",
    "\n",
    "We just claimed that cross entropy always overestimates the true entropy. Equivallently the KL Divergence is always positive. Since the proof of this is short and sweet I'll include it here.\n",
    "\n",
    "First note the taylor expansion of $\\log_e$ when $|x| < 1$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\log_e (x) &= -\\log_e(1 - (1-x)) \\\\\n",
    "           &= \\sum_{n=1}^{\\infty} \\frac{(1-x)^n}{n!} \\\\\n",
    "           &\\ge (1-x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{D}_{KL}(P\\parallel Q) \n",
    "    &= -\\sum_j p_j \\log_2 \\frac{q_j}{p_j} \\\\\n",
    "    &\\ge \\sum_j p_j \\left(1 - \\frac{q_j}{p_j}\\right) \\\\\n",
    "    &= \\sum_j (p_j - q_j) \\\\\n",
    "    &= (1 - 1) = 0           \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With equality if and only if when $P = Q$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bits Revisited\n",
    "\n",
    "When we first introduced the idea of entropy as the number of bits of information, we asked why encoding the flips of a weighted coin the same way as a fair coin gave us the wrong intuition about entropy. Well, now we can see that encoding the weighted coin like the fair coin, would be to assume the incorrect distribution of the coin.\n",
    "\n",
    "That is, while the entropy of $n$ flips of the weighted coin, $P$, is\n",
    "\n",
    "$$\\mathcal{H}(P) = n(-0.9\\log_2(0.9)-0.1\\log_2(0.1)) \\approx 0.47n$$\n",
    "\n",
    "If we had assumed the incorrect distribution of a fair coin, $Q$,\n",
    "\n",
    "$$\\mathcal{H}(P, Q) = n(-0.9\\log_2(0.5)-0.1\\log_2(0.5)) = n$$\n",
    "\n",
    "That is if we encode the $n$ flips with $n$ bits then we used $\\mathcal{D}(P \\parallel Q) \\approx n - 0.47n = 0.53n$ too many bits. Or equivalently, we overestimated the entropy by $0.53n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and Machine Learning\n",
    "\n",
    "Ok, this is great and all, but how does one apply the idea of cross entropy or KL divergence to ML? \n",
    "\n",
    "One place where cross entropy can be found is in the loss function of a classification model.\n",
    "\n",
    "Suppose a model, $M$, is predicting one of $n$ classes $C_1, C_2,...,C_n$. For one observation $Y=C_k$ and data $X$, the model predicts $\\hat{Y}=M(X)$, where $\\hat{Y}$ is a vector of probability masses summing to $1$ and representing the weight that the model places on each of the $n$ classes. The _true distribution_ is the one-hot-encoding of $Y$ (meaning a vector with a $1$ in position $k$ and $0$ elsewhere), which we'll denote $Y^*$. The cross entropy is then calculated as:\n",
    "\n",
    "$$\\mathcal{H}(Y^*,\\hat{Y}) = - \\sum_j y^*_j \\log \\hat{y}_j = -\\log y_k$$\n",
    "\n",
    "Where it is understood that $p \\log p = 0$ when $p=0$ because $x$ dominates $\\log x$.\n",
    "\n",
    "The idea is that the this is the loss function that you minimise while training the model. Note that because $Y^*$ is the distribution that says the class is definitely $C_k$, then there is no uncertainty and the entropy of $Y^*$ is $0$. So in this case the cross entropy is also the KL divergence.\n",
    "\n",
    "Also note that because this all boils down to taking a single log, this is also called the log loss.\n",
    "\n",
    "Man, that was a long way to come to discover that Cross Entropy loss is just log loss. Let's see a more interesting example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain and Decision Trees\n",
    "\n",
    "> Mathematicans are low entropy. They take lots of abstract ideas, relate them and give it all a single name.\n",
    "> Data Scientists are high entropy. They take one idea and give it 20 different names.\n",
    "\n",
    "To that quip, Information Gain is basically just another name for KL Divergence used with reference to decision trees.\n",
    "\n",
    "Suppose we are building a decision tree $T$, that predicts one of $n$ classes $C_1, C_2, ..., C_n$ from $m$ predictors. \n",
    "\n",
    "Our data is $N$ observations $y=C_k$, each with $m$ predictors $(x_1, x_2, ..., x_m)$, where each predictor $x_j$ is one of $X_j$ classes (e.g. the first predictor is sex: $x_1=\\text{female}$ and $X_1 = \\{\\text{male, female} \\}$).\n",
    "\n",
    "To determine which predictor should split on at the next node of the decision tree, you select the one that decreases entropy the most (i.e. has the highest information gain).\n",
    "\n",
    "To compute the information gain for class $X_j$, we take the difference between the entropy of all observations $\\mathcal{H}(Y)$, and the weighted sum of the entropies over the $|X_j|$ different partitions of $Y$.\n",
    "\n",
    "Let's do this one with an explicit example. We want to predict a child's sport preference based on their height and their sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preference</th>\n",
       "      <th>height</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>soccer</td>\n",
       "      <td>short</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soccer</td>\n",
       "      <td>short</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>soccer</td>\n",
       "      <td>short</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soccer</td>\n",
       "      <td>short</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soccer</td>\n",
       "      <td>short</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>soccer</td>\n",
       "      <td>short</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>soccer</td>\n",
       "      <td>short</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>soccer</td>\n",
       "      <td>tall</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>soccer</td>\n",
       "      <td>tall</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>soccer</td>\n",
       "      <td>tall</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>basketball</td>\n",
       "      <td>tall</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    preference height     sex\n",
       "0       soccer  short    male\n",
       "1       soccer  short  female\n",
       "2       soccer  short    male\n",
       "3       soccer  short  female\n",
       "4       soccer  short    male\n",
       "5       soccer  short  female\n",
       "6       soccer  short    male\n",
       "7       soccer   tall  female\n",
       "8       soccer   tall    male\n",
       "9       soccer   tall  female\n",
       "10  basketball   tall    male\n",
       "11  basketball   tall  female\n",
       "12  basketball   tall    male\n",
       "13  basketball   tall  female\n",
       "14  basketball   tall    male\n",
       "15  basketball   tall  female\n",
       "16  basketball   tall    male\n",
       "17  basketball   tall  female\n",
       "18  basketball   tall    male\n",
       "19  basketball   tall  female"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# some contrived data\n",
    "df = pd.DataFrame({\n",
    "    \"preference\": [\"soccer\"]*10 + [\"basketball\"]*10,\n",
    "    \"height\": [\"short\"]*7 + [\"tall\"]*13,\n",
    "    \"sex\": [\"male\", \"female\"]*10,\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just look at the data it seems evident that taller children like basketball, and sex has no impact at all. So we should expect that the information gain from splitting on height is high, and the gain from splitting on sex should be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG Height: 0.49342260576014463\n",
      "IG Gender: 0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def H(df: pd.DataFrame, response: str) -> float:\n",
    "    \"\"\" compute the entropy of the empirical distriution generated from\n",
    "        the observations in column `response` of the DataFrame\n",
    "    \"\"\"\n",
    "    probabilities = df[response].value_counts() / len(df)\n",
    "    return sum(-p*math.log(p, 2) for p in probabilities)\n",
    "\n",
    "\n",
    "def H_conditional(df: pd.DataFrame, response: str, attribute: str) -> float:\n",
    "    \"\"\" compute the average entropy of the empirical distriutions generated \n",
    "        from the observations in column `response` split by `attribute`\n",
    "    \"\"\"\n",
    "    N = len(df)\n",
    "    return sum(\n",
    "        (len(group) / N) * H(group, response)\n",
    "        for _, group in df.groupby(attribute)\n",
    "    )\n",
    "\n",
    "\n",
    "def InformationGain(df: pd.DataFrame, response: str, attribute: str) -> float:\n",
    "    \"\"\" compute the information gain from splitting on `attribute` \"\"\"\n",
    "    return H(df, response) - H_conditional(df, response, attribute)\n",
    "\n",
    "\n",
    "print(\"IG Height:\", InformationGain(df, \"preference\", \"height\"))\n",
    "print(\"IG Gender:\", InformationGain(df, \"preference\", \"sex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the splitting on height reduces entropy by $0.49$ and splitting on sex does not reduce entropy at all, which is in line with out expectation. Note that because an equal number of boys and girls both like soccer and basketball, you do not gain any information about their sport preference if you know the sex of the child. That is, the information gain is zero.\n",
    "\n",
    "In our decision tree, we would therefore split on height because it provided the largest decrease in entropy / decrease in uncertainty / KL divergence / information gain, or however you like to say it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Entropy\n",
    "\n",
    "This tutorial wouldn't be complete if we didn't touch on the concept of maximum entropy. We noted previously that the information/entropy associated with a single event $E$ does not depend on the sample space. While this is true for single events, it is not true when we talk about the average entropy of a random variable.\n",
    "\n",
    "When there are more possible outcomes, the average uncertainty (i.e. entropy) is higher \n",
    "\n",
    "Indeed, consider the entropy of $U(1, n)$,\n",
    "\n",
    "$$\\mathcal{H}(U(1,n)) = -\\sum_{j=1}^n \\frac{1}n \\log_2\\frac{1}n = \\log_2 n$$\n",
    "\n",
    "Which will tend to infinity as $n \\to \\infty$.\n",
    "\n",
    "When we talk about maximum entropy we do so in relation to some constraint. \n",
    "\n",
    "For instance, when we restrict the sample space to have cardinality $n$ then the distribution with maxmimum entropy is the uniform distribution. That is $\\mathcal{H}(X)$ is maximised when all the terms $p_j$ are equal.\n",
    "\n",
    "This also coincides with the physics interpretation of maximum entropy in which particles are very dispersed (uniformly distributed) through space.\n",
    "\n",
    "Now, if were talking about a continuous distribution and we knew the mean and variance, then the maximum entropy distribution would be the Gaussian!\n",
    "\n",
    "Which brings us to our final point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Distributions\n",
    "\n",
    "In this tutorial we have only considered discrete distributions. How do we extend entropy to continuous ones? We would hope that we can simply write\n",
    "\n",
    "$$\\mathcal{H}(X) = -\\int_{\\mathbb{R}} f_X(x) \\log f_X(x)\\, dx$$\n",
    "\n",
    "Unfortunately most of our intuition breaks down when we try to replace probability masses with probability densities. For one, a probability density can be greater than $1$, which would give us a negative entropy at that point. At least from an Machine Learning perspective, this is one reason why we see entropy as a loss function in classification problems and generally not in regression problems.\n",
    "\n",
    "As it turns out, with some careful consideration, the continuous case can be made to work. But I will not go into that here because this tutorial is already quite long and has hopefully acheived my goal of providing an intuition for Entropy, with a little bit of math and rigour."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
